import torch
import os
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. è·¯å¾„è®¾ç½®
BASE_PATH = "/root/autodl-tmp"
DB_PATH = os.path.join(BASE_PATH, "chroma_db")
MODEL_CACHE = os.path.join(BASE_PATH, "models")

# 2. æ¨¡å‹ä¸‹è½½
print("æ­£åœ¨é€šè¿‡ ModelScope ä¸‹è½½æ¨¡å‹...")
# ä¸‹è½½ LLM
llm_model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir=MODEL_CACHE)
# ä¸‹è½½ Embedding æ¨¡å‹ (ä¿®å¤ OSError çš„å…³é”®)
embed_model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir=MODEL_CACHE)

# 3. åŠ è½½ LLM
print("æ­£åœ¨åŠ è½½ Qwen2.5-7B åˆ° RTX 5090...")
tokenizer = AutoTokenizer.from_pretrained(llm_model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    llm_model_dir,
    device_map="auto",
    torch_dtype=torch.bfloat16, 
    trust_remote_code=True
)

# 4. åŠ è½½ Embedding (æŒ‡å‘æœ¬åœ°è·¯å¾„)
print("æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹...")
embeddings = HuggingFaceEmbeddings(
    model_name=embed_model_dir, # ä½¿ç”¨åˆšæ‰ä¸‹è½½å¥½çš„æœ¬åœ°è·¯å¾„
    model_kwargs={'device': 'cuda'}
)

# 5. è¿æ¥æ•°æ®åº“
if not os.path.exists(DB_PATH):
    print(f"âŒ é”™è¯¯ï¼šåœ¨ {DB_PATH} æœªæ‰¾åˆ°æ•°æ®åº“ï¼Œè¯·å…ˆè¿è¡Œ build_db.py")
    exit()
    
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 3})

# 6. æ¨ç† Pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9
)
llm = HuggingFacePipeline(pipeline=pipe)

# 7. LCEL RAG é“¾
template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
èµ„æ–™åº“ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

if __name__ == "__main__":
    test_query = "å¤´ç—›æ¶å¿ƒè‚Œè‚‰ç—›å…³èŠ‚ç—›æ€ä¹ˆå›äº‹ï¼Ÿ"
    print(f"\nğŸš€ å¯åŠ¨æ£€ç´¢é—®ç­”...\næé—®ï¼š{test_query}")
    try:
        response = rag_chain.invoke(test_query)
        print(f"\nâœ… AIå›ç­”ï¼š\n{response}")
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™ï¼š{e}")