import gradio as gr
import torch
import os
import gc
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- 1. ç¯å¢ƒä¸æ¨¡å‹é…ç½® ---
BASE_PATH = "/root/autodl-tmp"
DB_PATH = os.path.join(BASE_PATH, "chroma_db")
MODEL_CACHE = os.path.join(BASE_PATH, "models")
MERGED_MODEL_PATH = "/root/autodl-tmp/output/qwen2_5-7b-medical-loraâ€”pro/v0-20260110-211358/checkpoint-45"

print("æ­£åœ¨å¯åŠ¨æ™ºåŒ» RAG ç³»ç»Ÿ (ç»“æ„åŒ–è¾“å‡ºå¢å¼ºç‰ˆ)...")

# åŠ è½½æ¨¡å‹
embed_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir=MODEL_CACHE)

tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MERGED_MODEL_PATH, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True
)

# åˆå§‹åŒ– RAG æ£€ç´¢å™¨
embeddings = HuggingFaceEmbeddings(model_name=embed_dir, model_kwargs={'device': 'cuda'})
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 50})

# --- 2. æ¨ç†å¼•æ“ï¼šç»“æ„åŒ– Prompt é›†æˆ ---
def chat_and_retrieve(message):
    print(f"\nğŸ” æ”¶åˆ°ç”¨æˆ·æé—®: {message}")  
    
    try:
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ‰§è¡Œæ·±åº¦æ£€ç´¢ï¼Œæ»¡è¶³æ€§èƒ½æŒ‡æ ‡
        docs = retriever.invoke(message)
        print(f"âœ… æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(docs)} æ¡ç›¸å…³èµ„æ–™")

        context = "\n".join([f"[{i+1}] {d.page_content}" for i, d in enumerate(docs)])
        
        source_display_content = "### ğŸ“š æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™\n\n"
        if not docs:
            source_display_content += "âš ï¸ æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³åŒ¹é…å†…å®¹ã€‚"
        
        for i, d in enumerate(docs[:5]): # åªå–å‰ 5 æ¡å±•ç¤º
            source_display_content += f"### ğŸ“ æ ¸å¿ƒèµ„æ–™æ¥æº {i+1}\n{d.page_content[:200]}...\n\n---\n"
        
        if len(docs) > 5:
            source_display_content += f"\n*æ³¨ï¼šåå°å·²æ£€ç´¢å¹¶åˆ†æå…¶ä½™ {len(docs)-5} æ¡è¾…åŠ©èµ„æ–™ä»¥ç¡®ä¿ç»“è®ºå‡†ç¡®ã€‚*"
        
        prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªåŒ»ç”Ÿã€‚è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
"""

        # æ¨ç†å‚æ•°ä¼˜åŒ–
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        gen_kwargs = dict(
            inputs, 
            streamer=streamer, 
            max_new_tokens=600,     
            temperature=0.3,        
            repetition_penalty=1.2, 
            top_p=0.9,
            do_sample=True
        )
        
        thread = Thread(target=model.generate, kwargs=gen_kwargs)
        thread.start()

        # å®æ—¶æµå¼å“åº”
        full_response = ""
        for new_text in streamer:
            # è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦
            clean_text = new_text.replace("<|im_end|>", "").replace("<|im_start|>", "")
            full_response += clean_text
            yield full_response.strip(), source_display_content
            
    except Exception as e:
        yield f"âš ï¸ ç³»ç»Ÿè¯Šæ–­é”™è¯¯: {str(e)}", "æ£€ç´¢å¤±è´¥"

# --- 3. Gradio äº¤äº’ç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft(primary_hue="green"), title="æ™ºåŒ» RAG é—®ç­”å¹³å°") as demo:
    gr.Markdown("# ğŸ¥ æ™ºåŒ» RAGï¼šä¸­æ–‡åŒ»ç–—é—®ç­”å¹³å°")
    
    with gr.Row():
        with gr.Column(scale=7):
            chatbot = gr.Chatbot(label="AI åŒ»ç”ŸåŠ©æ‰‹", height=550)
            msg = gr.Textbox(label="è¾“å…¥æ‚¨çš„ç–‘é—®ï¼ˆå¦‚ï¼šå¤´ç–¼æ€ä¹ˆåŠï¼Ÿï¼‰", placeholder="è¾“å…¥åæŒ‰å›è½¦å‘é€...")
            with gr.Row():
                submit_btn = gr.Button("ğŸš€ æäº¤å’¨è¯¢", variant="primary")
                clear_btn = gr.Button("ğŸ—‘ï¸ é‡ç½®å¯¹è¯")

        with gr.Column(scale=3):
            gr.Markdown("### ğŸ” çŸ¥è¯†åº“æ£€ç´¢æº¯æº")
            sources_display = gr.Markdown("ç­‰å¾…æ£€ç´¢èµ„æ–™...", label="å‚è€ƒèµ„æ–™")

    # äº¤äº’é€»è¾‘é€‚é… Gradio 4.x/5.x çš„å­—å…¸æ ¼å¼æ ¡éªŒ
    def respond(user_message, chat_history):
        if chat_history is None:
            chat_history = []
        
        chat_history.append({"role": "user", "content": user_message})
        chat_history.append({"role": "assistant", "content": ""})
        
        response_gen = chat_and_retrieve(user_message)
        
        for chat_text, source_text in response_gen:
            chat_history[-1]["content"] = chat_text
            yield chat_history, source_text

    # äº‹ä»¶æµç»‘å®š
    submit_btn.click(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    msg.submit(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    clear_btn.click(lambda: (None, "ç­‰å¾…æé—®..."), None, [chatbot, sources_display])

if __name__ == "__main__":
    # ä½¿ç”¨ AutoDL æ˜ å°„ç«¯å£ 6006 è®¿é—®
    print("âœ… æœåŠ¡å³å°†åœ¨ http://127.0.0.1:6006 å¯åŠ¨")
    demo.launch(server_name="0.0.0.0", server_port=6006, share=False)