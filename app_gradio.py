import gradio as gr
import torch
import os
import gc
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- 1. ç¯å¢ƒä¸æ¨¡å‹é…ç½® ---
BASE_PATH = "/root/autodl-tmp"
DB_PATH = os.path.join(BASE_PATH, "chroma_db")
MODEL_CACHE = os.path.join(BASE_PATH, "models")

print("æ­£åœ¨å¯åŠ¨æ™ºåŒ» RAG ç³»ç»Ÿ (ç»“æ„åŒ–è¾“å‡ºå¢å¼ºç‰ˆ)...")

# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
llm_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir=MODEL_CACHE)
embed_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir=MODEL_CACHE)

tokenizer = AutoTokenizer.from_pretrained(llm_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    llm_dir, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True
)

# åˆå§‹åŒ– RAG æ£€ç´¢å™¨
embeddings = HuggingFaceEmbeddings(model_name=embed_dir, model_kwargs={'device': 'cuda'})
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 50})

# --- 2. æ¨ç†å¼•æ“ï¼šç»“æ„åŒ– Prompt é›†æˆ ---
def chat_and_retrieve(message):
    print(f"\nğŸ” æ”¶åˆ°ç”¨æˆ·æé—®: {message}")  
    
    try:
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ‰§è¡Œæ·±åº¦æ£€ç´¢ï¼Œæ»¡è¶³æ€§èƒ½æŒ‡æ ‡
        docs = retriever.invoke(message)
        print(f"âœ… æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(docs)} æ¡ç›¸å…³èµ„æ–™")

        # æ„é€ ç»™æ¨¡å‹çœ‹çš„å…¨æ–‡ï¼ˆå¯èƒ½åŒ…å« 32k tokensï¼‰
        context = "\n".join([f"[{i+1}] {d.page_content}" for i, d in enumerate(docs)])
        
        # æ„é€ ç»™ç”¨æˆ·çœ‹çš„ç®€ç‰ˆèµ„æ–™æº¯æºï¼ˆä»…å±•ç¤ºå‰ 5 æ¡ï¼‰
        source_display_content = "### ğŸ“š æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™\n\n"
        if not docs:
            source_display_content += "âš ï¸ æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³åŒ¹é…å†…å®¹ã€‚"
        
        for i, d in enumerate(docs[:5]): # åªå–å‰ 5 æ¡å±•ç¤º
            source_display_content += f"### ğŸ“ æ ¸å¿ƒèµ„æ–™æ¥æº {i+1}\n{d.page_content[:200]}...\n\n---\n"
        
        if len(docs) > 5:
            source_display_content += f"\n*æ³¨ï¼šåå°å·²æ£€ç´¢å¹¶åˆ†æå…¶ä½™ {len(docs)-5} æ¡è¾…åŠ©èµ„æ–™ä»¥ç¡®ä¿ç»“è®ºå‡†ç¡®ã€‚*"
        
        # b. ã€é«˜æ ‡å‡†å›å¤ã€‘é›†æˆç»“æ„åŒ–æŒ‡ä»¤ä¸çŸ¥è¯†æ‰©å±•çš„ Prompt
        prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæåº¦ä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·åœ¨å‚è€ƒèµ„æ–™çš„åŸºç¡€ä¸Šï¼Œç»™å‡ºå…·æœ‰å®æ“æ„ä¹‰ã€æ¡ç†æ¸…æ™°çš„åŒ»ç–—å»ºè®®ã€‚

ã€é‡è¦æç¤ºã€‘ï¼š
1. å‚è€ƒèµ„æ–™å·²æŒ‰ç›¸å…³æ€§æ’åºï¼Œè¯·ä¼˜å…ˆå‚è€ƒé å‰çš„æ ¸å¿ƒèµ„æ–™ã€‚
2. è‹¥èµ„æ–™é‡è¾ƒå¤§ä¸”åŒ…å«æ— å…³å¹²æ‰°ï¼Œè¯·æœæ–­å¿½ç•¥ï¼Œä¸¥ç¦äº§ç”Ÿå¹»è§‰ã€‚

ã€å›ç­”ç­–ç•¥ã€‘ï¼š
1. èƒŒæ™¯åˆ†æï¼šç®€è¦è¯´æ˜ç—‡çŠ¶å¯èƒ½çš„åŸå› ã€‚
2. ç¼“è§£æ–¹æ¡ˆï¼šä½¿ç”¨æ•°å­—åˆ—è¡¨ç»™å‡ºå…·ä½“æªæ–½ã€‚
3. è¯ç‰©æŒ‡å¯¼ï¼šæåŠå¸¸ç”¨éå¤„æ–¹è¯å¹¶å¼ºè°ƒéµåŒ»å˜±ã€‚
4. è­¦ç¤ºè¯´æ˜ï¼šæé†’åŠæ—¶å°±åŒ»ã€‚
5. ä¸“ä¸šåç¼€ï¼šç»“å°¾å›ºå®šåŒ…å«â€œä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå®é™…æ“ä½œæ—¶è¯·éµå¾ªä¸“ä¸šåŒ»ç”Ÿçš„æ„è§ã€‚â€<|im_end|>
<|im_start|>user
å‚è€ƒèµ„æ–™å†…å®¹ï¼š
{context}

ç”¨æˆ·å’¨è¯¢é—®é¢˜ï¼š{message}<|im_end|>
<|im_start|>assistant
"""

        # c. æ¨ç†å‚æ•°ä¼˜åŒ–
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        gen_kwargs = dict(
            inputs, 
            streamer=streamer, 
            max_new_tokens=600,     # ç¨å¾®è°ƒå¤§å­—æ•°ä¸Šé™ï¼Œå…è®¸æ¨¡å‹å†™å‡ºæ›´è¯¦ç»†çš„å»ºè®®
            temperature=0.3,        # ç¨å¾®æå‡ä¸€ç‚¹æ¸©åº¦ï¼ˆä»0.2åˆ°0.4ï¼‰ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸“ä¸šèŒƒå›´å†…è¿›è¡Œåˆç†çš„è¯­è¨€æ¶¦è‰²
            repetition_penalty=1.2, # é™ä½æƒ©ç½šåŠ›åº¦ï¼Œé˜²æ­¢æ¨¡å‹å› ä¸ºæ€•é‡å¤è€Œä¸æ•¢å†™å‡ºç»“æ„ç›¸ä¼¼çš„å»ºè®®
            top_p=0.9,
            do_sample=True
        )
        
        thread = Thread(target=model.generate, kwargs=gen_kwargs)
        thread.start()

        # d. å®æ—¶æµå¼å“åº”
        full_response = ""
        for new_text in streamer:
            # è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦
            clean_text = new_text.replace("<|im_end|>", "").replace("<|im_start|>", "")
            full_response += clean_text
            yield full_response.strip(), source_display_content
            
    except Exception as e:
        yield f"âš ï¸ ç³»ç»Ÿè¯Šæ–­é”™è¯¯: {str(e)}", "æ£€ç´¢å¤±è´¥"

# --- 3. Gradio äº¤äº’ç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft(primary_hue="green"), title="æ™ºåŒ» RAG é—®ç­”å¹³å°") as demo:
    gr.Markdown("# ğŸ¥ æ™ºåŒ» RAGï¼šä¸­æ–‡åŒ»ç–—é—®ç­”å¹³å°")
    
    with gr.Row():
        with gr.Column(scale=7):
            chatbot = gr.Chatbot(label="AI åŒ»ç”ŸåŠ©æ‰‹", height=550)
            msg = gr.Textbox(label="è¾“å…¥æ‚¨çš„ç–‘é—®ï¼ˆå¦‚ï¼šå¤´ç–¼æ€ä¹ˆåŠï¼Ÿï¼‰", placeholder="è¾“å…¥åæŒ‰å›è½¦å‘é€...")
            with gr.Row():
                submit_btn = gr.Button("ğŸš€ æäº¤å’¨è¯¢", variant="primary")
                clear_btn = gr.Button("ğŸ—‘ï¸ é‡ç½®å¯¹è¯")

        with gr.Column(scale=3):
            gr.Markdown("### ğŸ” çŸ¥è¯†åº“æ£€ç´¢æº¯æº")
            sources_display = gr.Markdown("ç­‰å¾…æ£€ç´¢èµ„æ–™...", label="å‚è€ƒèµ„æ–™")

    # äº¤äº’é€»è¾‘é€‚é… Gradio 4.x/5.x çš„å­—å…¸æ ¼å¼æ ¡éªŒ
    def respond(user_message, chat_history):
        if chat_history is None:
            chat_history = []
        
        chat_history.append({"role": "user", "content": user_message})
        chat_history.append({"role": "assistant", "content": ""})
        
        response_gen = chat_and_retrieve(user_message)
        
        for chat_text, source_text in response_gen:
            chat_history[-1]["content"] = chat_text
            yield chat_history, source_text

    # äº‹ä»¶æµç»‘å®š
    submit_btn.click(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    msg.submit(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    clear_btn.click(lambda: (None, "ç­‰å¾…æé—®..."), None, [chatbot, sources_display])

if __name__ == "__main__":
    # ä½¿ç”¨ AutoDL æ˜ å°„ç«¯å£ 6006 è®¿é—®
    print("âœ… æœåŠ¡å³å°†åœ¨ http://127.0.0.1:6006 å¯åŠ¨")
    demo.launch(server_name="0.0.0.0", server_port=6006, share=False)