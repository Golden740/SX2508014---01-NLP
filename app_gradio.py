import gradio as gr
import torch
import os
import gc
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- 1. èµ„æºä¸è·¯å¾„åˆå§‹åŒ– ---
BASE_PATH = "/root/autodl-tmp"
DB_PATH = os.path.join(BASE_PATH, "chroma_db")
MODEL_CACHE = os.path.join(BASE_PATH, "models")

print("æ­£åœ¨åˆå§‹åŒ–æ™ºåŒ» RAG ç³»ç»Ÿ (ç²¾ç®€å›å¤ç‰ˆ)...")

# ä¸‹è½½/åŠ è½½æ¨¡å‹
llm_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir=MODEL_CACHE)
embed_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir=MODEL_CACHE)

# åŠ è½½ Tokenizer å’Œ Model
tokenizer = AutoTokenizer.from_pretrained(llm_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    llm_dir, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True
)

# åŠ è½½ RAG æ£€ç´¢å™¨
embeddings = HuggingFaceEmbeddings(model_name=embed_dir, model_kwargs={'device': 'cuda'})
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 3})

# --- 2. æ ¸å¿ƒæ¨ç†é€»è¾‘ ---
def chat_and_retrieve(message):
    try:
        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()

        # a. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = retriever.invoke(message)
        source_content = ""
        context_parts = []
        for i, d in enumerate(docs):
            source_content += f"### ğŸ“ èµ„æ–™æ¥æº {i+1}\n{d.page_content}\n\n---\n"
            context_parts.append(d.page_content)
        
        context = "\n\n".join(context_parts)
        
        # b. æ„é€ ä¸¥æ ¼çš„ ChatML æ ¼å¼ Prompt
        # è¿™ç§æ ¼å¼èƒ½æœ‰æ•ˆé˜²æ­¢ Qwen æ¨¡å‹å¤è¿° Prompt é‡Œçš„èµ„æ–™æ ‡ç­¾
        prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„èµ„æ–™è¿›è¡Œæ€»ç»“å›ç­”ã€‚
è¦æ±‚ï¼š
- ç›´æ¥ç»™å‡ºåŒ»ç–—å»ºè®®æˆ–ç»“è®ºï¼Œç¦æ­¢åŸæ ·å¤è¿°å‚è€ƒèµ„æ–™ä¸­çš„â€œç”¨æˆ·é—®é¢˜â€æˆ–â€œåŒ»ç”Ÿå»ºè®®â€ã€‚
- è¯­è¨€ç²¾ç‚¼ï¼Œç¦æ­¢å‡ºç°é‡å¤çš„æ®µè½ã€‚
- è‹¥èµ„æ–™æ— å…³ï¼Œè¯·åŸºäºä¸“ä¸šåŒ»å­¦çŸ¥è¯†å›ç­”å¹¶æç¤ºå»ºè®®ä»…ä¾›å‚è€ƒã€‚<|im_end|>
<|im_start|>user
å‚è€ƒèµ„æ–™å†…å®¹ï¼š
{context}

ç”¨æˆ·å’¨è¯¢é—®é¢˜ï¼š{message}<|im_end|>
<|im_start|>assistant
"""

        # c. å‡†å¤‡æ¨ç†è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # d. ç”Ÿæˆå‚æ•°ä¼˜åŒ– (å…³é”®ï¼šä½æ¸©åº¦ + é‡å¤æƒ©ç½š)
        gen_kwargs = dict(
            inputs, 
            streamer=streamer, 
            max_new_tokens=512, 
            temperature=0.3,      # é™ä½éšæœºæ€§
            repetition_penalty=1.2, # æƒ©ç½šé‡å¤å†…å®¹
            top_p=0.8
        )
        
        thread = Thread(target=model.generate, kwargs=gen_kwargs)
        thread.start()

        # e. æµå¼è¿­ä»£è¾“å‡º
        full_response = ""
        for new_text in streamer:
            # è¿‡æ»¤å¯èƒ½å‡ºç°çš„åœæ­¢ç¬¦
            clean_text = new_text.replace("<|im_end|>", "").replace("<|im_start|>", "")
            full_response += clean_text
            # ä»…å‘å‰ç«¯è¿”å›æ¨¡å‹ç”Ÿæˆçš„çº¯å‡€å›ç­”å’Œæº¯æºèµ„æ–™
            yield full_response.strip(), source_content
            
    except Exception as e:
        yield f"âš ï¸ ç³»ç»Ÿç¹å¿™: {str(e)}", "æ£€ç´¢å¤±è´¥"

# --- 3. Gradio ç•Œé¢è®¾è®¡ (é€‚é…å­—å…¸æ ¼å¼æ ¡éªŒ) ---
with gr.Blocks(theme=gr.themes.Soft(primary_hue="green"), title="æ™ºåŒ» RAG") as demo:
    gr.Markdown("# ğŸ¥ æ™ºåŒ» RAGï¼šåŒ»ç–—é—®ç­”å¹³å°")
    gr.Markdown("æç¤ºï¼šç³»ç»Ÿä¼šè‡ªåŠ¨ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ç—…ä¾‹å¹¶ç”± AI è¿›è¡Œæ€»ç»“å›å¤ã€‚")
    
    with gr.Row():
        # å¯¹è¯åŒº
        with gr.Column(scale=7):
            chatbot = gr.Chatbot(label="åŒ»ç”ŸåŠ©æ‰‹å¯¹è¯æ¡†", height=550)
            msg = gr.Textbox(label="æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–é—®é¢˜", placeholder="è¾“å…¥åæŒ‰å›è½¦å‘é€...")
            with gr.Row():
                submit_btn = gr.Button("ğŸš€ æäº¤å’¨è¯¢", variant="primary")
                clear_btn = gr.Button("ğŸ—‘ï¸ é‡ç½®å¯¹è¯")

        # èµ„æ–™åŒº
        with gr.Column(scale=3):
            gr.Markdown("### ğŸ” çŸ¥è¯†åº“æ£€ç´¢æº¯æº")
            sources_display = gr.Markdown("ç­‰å¾…æé—®ä»¥æ˜¾ç¤ºå‚è€ƒèµ„æ–™...", label="å‚è€ƒèµ„æ–™")

    # --- äº¤äº’ç»‘å®š ---
    def respond(user_message, chat_history):
        if chat_history is None:
            chat_history = []
            
        # å°è£…ä¸ºæ–°ç‰ˆå­—å…¸æ ¼å¼
        chat_history.append({"role": "user", "content": user_message})
        chat_history.append({"role": "assistant", "content": ""})
        
        # è·å–ç”Ÿæˆå™¨è¾“å‡º
        response_gen = chat_and_retrieve(user_message)
        
        for chat_text, source_text in response_gen:
            chat_history[-1]["content"] = chat_text
            yield chat_history, source_text

    # äº‹ä»¶æµ
    submit_btn.click(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    msg.submit(respond, [msg, chatbot], [chatbot, sources_display]).then(lambda: "", None, [msg])
    clear_btn.click(lambda: (None, "ç­‰å¾…æ£€ç´¢..."), None, [chatbot, sources_display])

if __name__ == "__main__":
    # å…³é—­ share=True ä»¥é¿å¼€ frpc æŠ¥é”™ï¼Œä½¿ç”¨ AutoDL è‡ªå¸¦æ˜ å°„è®¿é—®
    demo.launch(server_name="0.0.0.0", server_port=6006, share=False)