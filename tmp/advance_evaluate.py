import torch
import os
import json
import jieba
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from rouge_chinese import Rouge
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from metric_utils import compute_additional_metrics  # å¯¼å…¥åˆšæ‰å†™çš„å·¥å…·

# --- é…ç½®åŒºåŸŸ ---
BASE_MODEL_PATH = "/root/autodl-tmp/models/qwen/Qwen2.5-7B-Instruct"
LORA_PATH = "/root/autodl-tmp/output/qwen2_5-7b-medical-loraâ€”pro/v0-20260110-211358/checkpoint-45" 
DB_PATH = "/root/autodl-tmp/chroma_db"
TEST_FILE = "/root/autodl-tmp/medical_sft_pro_test.jsonl"
OUTPUT_FILE = "lora_evaluation_report.json"

def load_models():
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH, 
        torch_dtype=torch.bfloat16, 
        device_map="auto", 
        trust_remote_code=True
    )
    
    if LORA_PATH:
        print(f"âœ… æŒ‚è½½ LoRA æƒé‡: {LORA_PATH}")
        model = PeftModel.from_pretrained(model, LORA_PATH)
    else:
        print("â„¹ï¸ ä½¿ç”¨çº¯åŸºåº§æ¨¡å‹è¿›è¡Œè¯„ä¼°")
        
    model.eval()
    
    print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
    embeddings = HuggingFaceEmbeddings(
        model_name="/root/autodl-tmp/models/AI-ModelScope/bge-small-zh-v1.5",
        model_kwargs={'device': 'cuda'}
    )
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    return tokenizer, model, vector_db

def generate_response(model, tokenizer, question, context):
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç”Ÿã€‚åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š
èµ„æ–™ï¼š{context}
é—®é¢˜ï¼š{question}
è¯·ç»™å‡ºç»“æ„åŒ–çš„å›ç­”ï¼ˆåŒ…æ‹¬ç—…æƒ…åˆ†æã€æŒ‡å¯¼å»ºè®®ã€é£é™©æç¤ºï¼‰ã€‚"""
    
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(inputs.input_ids, max_new_tokens=512, temperature=0.3)
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response

def main():
    tokenizer, model, vector_db = load_models()
    rouge = Rouge()
    
    results = {
        "rouge-1": [], "rouge-2": [], "rouge-l": [],
        "accuracy": [], "citation_f1": [], "hallucination": []
    }
    
    # è¯»å–æµ‹è¯•é›†
    with open(TEST_FILE, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
        data = data[:50] 

    print(f"ğŸš€ å¼€å§‹è¯„ä¼° {len(data)} æ¡æ ·æœ¬...")
    
    for item in tqdm(data):
        query = item['instruction']
        reference = item['output']
        
        # 1. RAG æ£€ç´¢
        docs = vector_db.similarity_search(query, k=3)
        context_text = "\n".join([d.page_content for d in docs])
        
        # 2. æ¨¡å‹ç”Ÿæˆ
        prediction = generate_response(model, tokenizer, query, context_text)
        
        # 3. è®¡ç®— ROUGE
        prediction_seg = ' '.join(jieba.cut(prediction))
        reference_seg = ' '.join(jieba.cut(reference))
        try:
            scores = rouge.get_scores(prediction_seg, reference_seg)
            results['rouge-1'].append(scores[0]['rouge-1']['f'] * 100)
            results['rouge-2'].append(scores[0]['rouge-2']['f'] * 100)
            results['rouge-l'].append(scores[0]['rouge-l']['f'] * 100)
        except:
            pass 
            
        # 4. è®¡ç®—é«˜çº§æŒ‡æ ‡
        adv_metrics = compute_additional_metrics(prediction, reference, context_text)
        results['accuracy'].append(adv_metrics['accuracy'])
        results['citation_f1'].append(adv_metrics['citation_f1'])
        results['hallucination'].append(adv_metrics['hallucination'])

    # æ±‡æ€»æŠ¥å‘Š
    final_report = {k: round(np.mean(v), 2) for k, v in results.items()}
    print("\n" + "="*40)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š (Average Scores)")
    print("="*40)
    print(json.dumps(final_report, indent=4, ensure_ascii=False))
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=4)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()