import os
import json
import torch
import jieba
from tqdm import tqdm
from rouge_chinese import Rouge
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- 1. è·¯å¾„ä¸é…ç½® ---
BASE_PATH = "/root/autodl-tmp"
MODEL_PATH = os.path.join(BASE_PATH, "output/qwen2_5-7b-medical-lora/v0-20251230-233347/checkpoint-45-merged") 
DB_PATH = os.path.join(BASE_PATH, "chroma_db")
TEST_DATA_PATH = os.path.join(BASE_PATH, "medical_sft_pro_test.jsonl")

# è¯„ä¼°å‚æ•°
SAMPLE_NUM = 50  
K_VALUE = 50     

print(f"æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ (é•¿ä¸Šä¸‹æ–‡æ¨¡å¼ K={K_VALUE})...")

# --- 2. åŠ è½½ç»„ä»¶ ---
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH, 
    torch_dtype=torch.bfloat16, 
    device_map="auto", 
    trust_remote_code=True
)

embeddings = HuggingFaceEmbeddings(
    model_name=os.path.join(BASE_PATH, "models/AI-ModelScope/bge-small-zh-v1.5"),
    model_kwargs={'device': 'cuda'}
)
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": K_VALUE})

# --- 3. è¯„ä¼°æ ¸å¿ƒé€»è¾‘ ---
def run_evaluation():
    if not os.path.exists(TEST_DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶ {TEST_DATA_PATH}")
        return

    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        test_samples = [json.loads(line) for line in f][:SAMPLE_NUM]

    rouge = Rouge()
    preds, refs = [], []
    
    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªæåº¦ä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚è¯·åœ¨å‚è€ƒèµ„æ–™çš„åŸºç¡€ä¸Šï¼Œç»™å‡ºå…·æœ‰å®æ“æ„ä¹‰ã€æ¡ç†æ¸…æ™°çš„åŒ»ç–—å»ºè®®ã€‚\n\n"
        "ã€é‡è¦æç¤ºã€‘ï¼š\n"
        "1. å‚è€ƒèµ„æ–™å·²æŒ‰ç›¸å…³æ€§æ’åºï¼Œè¯·ä¼˜å…ˆå‚è€ƒé å‰çš„æ ¸å¿ƒèµ„æ–™ã€‚\n"
        "2. è‹¥èµ„æ–™é‡è¾ƒå¤§ä¸”åŒ…å«æ— å…³å¹²æ‰°ï¼Œè¯·æœæ–­å¿½ç•¥ï¼Œä¸¥ç¦äº§ç”Ÿå¹»è§‰ã€‚\n\n"
        "ã€å›ç­”ç­–ç•¥ã€‘ï¼š\n"
        "1. èƒŒæ™¯åˆ†æï¼šç®€è¦è¯´æ˜ç—‡çŠ¶å¯èƒ½çš„åŸå› ã€‚\n"
        "2. ç¼“è§£æ–¹æ¡ˆï¼šä½¿ç”¨æ•°å­—åˆ—è¡¨ç»™å‡ºå…·ä½“æªæ–½ã€‚\n"
        "3. è¯ç‰©æŒ‡å¯¼ï¼šæåŠå¸¸ç”¨éå¤„æ–¹è¯å¹¶å¼ºè°ƒéµåŒ»å˜±ã€‚\n"
        "4. è­¦ç¤ºè¯´æ˜ï¼šæé†’åŠæ—¶å°±åŒ»ã€‚\n"
        "5. ä¸“ä¸šåç¼€ï¼šç»“å°¾å›ºå®šåŒ…å«â€œä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå®é™…æ“ä½œæ—¶è¯·éµå¾ªä¸“ä¸šåŒ»ç”Ÿçš„æ„è§ã€‚â€"
    )

    print(f"ğŸš€ å¼€å§‹å¯¹ {SAMPLE_NUM} æ¡æ ·æœ¬è¿›è¡Œé•¿ä¸Šä¸‹æ–‡ RAG è¯„ä¼°...")

    for item in tqdm(test_samples):
        question = item.get('input', '')
        ground_truth = item.get('output', '')

        # a. æ¨¡æ‹Ÿé•¿ä¸Šä¸‹æ–‡æ£€ç´¢ (>32k tokens å‹åŠ›æµ‹è¯•)
        docs = retriever.invoke(question)
        context = "\n\n".join([d.page_content for d in docs])
        
        # b. æ„é€ å®Œæ•´æ¨ç† Prompt
        prompt = f"<|im_start|>system\n{system_instruction}<|im_end|>\n" \
                 f"<|im_start|>user\nå‚è€ƒèµ„æ–™å†…å®¹ï¼š\n{context}\n\nç”¨æˆ·å’¨è¯¢é—®é¢˜ï¼š{question}<|im_end|>\n" \
                 f"<|im_start|>assistant\n"
        
        # c. æ¨¡å‹ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=600, 
                temperature=0.3, 
                repetition_penalty=1.1
            )
        
        # d. æå–å›å¤å¹¶æ¸…ç†
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = full_output.split("assistant")[-1].strip()
        
        # e. åˆ†è¯ä»¥è®¡ç®—ä¸­æ–‡ Rouge
        preds.append(" ".join(jieba.cut(response)))
        refs.append(" ".join(jieba.cut(ground_truth)))

    # 4. è®¡ç®—å¹¶è¾“å‡ºç»“æœ
    scores = rouge.get_scores(preds, refs, avg=True)
    
    print("\n" + "="*50)
    print(f"ğŸ“Š æ™ºåŒ» RAG è¯„ä¼°æŠ¥å‘Š (æµ‹è¯•æ¨¡å¼: K={K_VALUE} é•¿ä¸Šä¸‹æ–‡)")
    print("="*50)
    print(f"ROUGE-1 (å•è¯è¦†ç›–ç‡): {scores['rouge-1']['f']:.4f}")
    print(f"ROUGE-2 (çŸ­è¯­åŒ¹é…åº¦): {scores['rouge-2']['f']:.4f}")
    print(f"ROUGE-L (è¯­ä¹‰é€»è¾‘å…³è”): {scores['rouge-l']['f']:.4f}")
    print("="*50)
    
    save_path = "lora_evaluation_results.json"
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(scores, f, indent=4)
    print(f"ç»“æœå·²ä¿å­˜è‡³: {save_path}")

if __name__ == "__main__":
    run_evaluation()