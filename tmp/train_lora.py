import torch.utils._pytree as pytree
import os

# 1. æ ¸å¿ƒè¡¥ä¸ (é’ˆå¯¹ Torch 2.9)
if not hasattr(pytree, 'register_pytree_node'):
    pytree.register_pytree_node = pytree._register_pytree_node

# 2. å¯¼å…¥ Swift 3.x è®­ç»ƒç±»
from swift.llm import TrainArguments, sft_main

# 3. é…ç½®è®­ç»ƒå‚æ•° (ä¸¥æ ¼å¯¹é½ Swift 3.x è§„èŒƒ)
sft_args = TrainArguments(
    # --- æ¨¡å‹ä¸è·¯å¾„ ---
    model='/root/autodl-tmp/models/qwen/Qwen2.5-7B-Instruct',
    train_type='lora',
    template='qwen',
    
    # --- æ•°æ®é›† ---
    dataset=['/root/autodl-tmp/medical_sft_pro_train.jsonl'],
    val_dataset=['/root/autodl-tmp/medical_sft_pro_test.jsonl'],
    
    # --- æ˜¾å­˜ä¸è®¡ç®— ---
    max_length=2048,
    gradient_checkpointing=True,
    
    # âœ… ä¿®æ­£æŠ¥é”™ï¼šSwift 3.x å°† batch_size æ‹†åˆ†ä¸ºä»¥ä¸‹ä¸¤ä¸ªå‚æ•°
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,
    
    # --- LoRA æ ¸å¿ƒå‚æ•° (å·²ä¿®æ­£å) ---
    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'], # åˆ æ‰ lora_ å‰ç¼€
    lora_rank=8,
    lora_alpha=32,
    lora_dropout=0.05,

    # --- ğŸš€ æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ ä»¥ä¸‹ä¸‰è¡Œ ---
    eval_steps=5,               # 2. æ¯ 10 æ­¥å°±è·‘ä¸€æ¬¡éªŒè¯é›†ï¼ˆå»ºè®®ä¸ logging_steps ä¸€è‡´ï¼Œè¿™æ ·ç‚¹æœ€å¯†é›†ï¼‰
    logging_steps=5,            # 3. æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒ Loss
    
    # --- è®­ç»ƒç­–ç•¥ ---
    learning_rate=1e-4,
    num_train_epochs=3,
    output_dir='output/qwen2_5-7b-medical-loraâ€”pro',
    
    # --- æ—¥å¿—ä¸ä¿å­˜ ---
    save_steps=50,
)

if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ Swift 3.x è®­ç»ƒå¼•æ“...")
    sft_main(sft_args)